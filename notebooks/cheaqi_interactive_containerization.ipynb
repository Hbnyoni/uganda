{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2e0a3073",
      "metadata": {},
      "source": [
        "# CHEAQI Spatial Interpolation Workbench\n",
        "This notebook provides an interactive workflow for transforming CSV environmental observations into gridded rasters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "880bbbe6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import math\n",
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "import tempfile\n",
        "import traceback\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Sequence, Tuple\n",
        "\n",
        "import ipywidgets as widgets\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import clear_output, display\n",
        "from pyproj import CRS, Transformer\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "try:\n",
        "    from osgeo import gdal, osr\n",
        "    HAS_GDAL_PY = True\n",
        "except Exception:\n",
        "    gdal = None\n",
        "    osr = None\n",
        "    HAS_GDAL_PY = False\n",
        "\n",
        "try:\n",
        "    from scipy.spatial import cKDTree\n",
        "    HAS_SCIPY = True\n",
        "except Exception:\n",
        "    cKDTree = None\n",
        "    HAS_SCIPY = False\n",
        "\n",
        "try:\n",
        "    from pykrige.ok import OrdinaryKriging\n",
        "    HAS_PYKRIGE = True\n",
        "except Exception:\n",
        "    OrdinaryKriging = None\n",
        "    HAS_PYKRIGE = False\n",
        "\n",
        "try:\n",
        "    import fiona\n",
        "    HAS_FIONA = True\n",
        "except Exception:\n",
        "    fiona = None\n",
        "    HAS_FIONA = False\n",
        "\n",
        "try:\n",
        "    import rasterio\n",
        "    from rasterio.transform import Affine\n",
        "    HAS_RASTERIO = True\n",
        "except Exception:\n",
        "    rasterio = None\n",
        "    Affine = None\n",
        "    HAS_RASTERIO = False\n",
        "\n",
        "try:\n",
        "    import xarray as xr\n",
        "    HAS_XARRAY = True\n",
        "except Exception:\n",
        "    xr = None\n",
        "    HAS_XARRAY = False\n",
        "\n",
        "GDAL_CMDS: Dict[str, Optional[str]] = {\n",
        "    \"gdal_grid\": shutil.which(\"gdal_grid\"),\n",
        "    \"gdalbuildvrt\": shutil.which(\"gdalbuildvrt\"),\n",
        "    \"gdal_translate\": shutil.which(\"gdal_translate\"),\n",
        "}\n",
        "GDAL_AVAILABLE = all(GDAL_CMDS.values())\n",
        "\n",
        "NODATA_VALUE = -9999.0\n",
        "MASTER_STACK_NAME = \"CHEAQI_master_stack.tif\"\n",
        "NETCDF_TIMESERIES_NAME = \"CHEAQI_timeseries.nc\"\n",
        "\n",
        "pd.options.mode.chained_assignment = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "00ebbf1b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def determine_utm_epsg(lon: float, lat: float) -> int:\n",
        "    zone = int((lon + 180.0) / 6.0) + 1\n",
        "    return 32600 + zone if lat >= 0 else 32700 + zone\n",
        "\n",
        "\n",
        "def prepare_dataframe(\n",
        "    df: pd.DataFrame,\n",
        "    lon_col: str,\n",
        "    lat_col: str,\n",
        "    date_col: str,\n",
        "    variable_cols: Sequence[str],\n",
        ") -> pd.DataFrame:\n",
        "    working = df.copy()\n",
        "    working[lon_col] = pd.to_numeric(working[lon_col], errors=\"coerce\")\n",
        "    working[lat_col] = pd.to_numeric(working[lat_col], errors=\"coerce\")\n",
        "    working[\"__cheaqi_date\"] = pd.to_datetime(working[date_col], errors=\"coerce\", utc=False)\n",
        "    working[\"__cheaqi_date\"] = working[\"__cheaqi_date\"].dt.tz_localize(None)\n",
        "    working = working.dropna(subset=[lon_col, lat_col, \"__cheaqi_date\"])\n",
        "    if working.empty:\n",
        "        raise ValueError(\"No valid rows remain after cleaning coordinates and dates.\")\n",
        "    working = working.copy()\n",
        "    for column in variable_cols:\n",
        "        working[column] = pd.to_numeric(working[column], errors=\"coerce\")\n",
        "    value_mask = working[variable_cols].notna().any(axis=1)\n",
        "    working = working.loc[value_mask]\n",
        "    if working.empty:\n",
        "        raise ValueError(\"Selected variables contain no numeric samples.\")\n",
        "    return working\n",
        "\n",
        "\n",
        "def reproject_dataframe(df: pd.DataFrame, lon_col: str, lat_col: str, target_epsg: int) -> pd.DataFrame:\n",
        "    transformer = Transformer.from_crs(\"EPSG:4326\", f\"EPSG:{target_epsg}\", always_xy=True)\n",
        "    xs, ys = transformer.transform(df[lon_col].to_numpy(), df[lat_col].to_numpy())\n",
        "    result = df.copy()\n",
        "    result[\"x\"] = xs\n",
        "    result[\"y\"] = ys\n",
        "    return result\n",
        "\n",
        "\n",
        "def read_aoi_bounds(aoi_path: Optional[str], target_epsg: int) -> Optional[Tuple[float, float, float, float]]:\n",
        "    if not aoi_path:\n",
        "        return None\n",
        "    path = Path(aoi_path).expanduser()\n",
        "    if not path.exists():\n",
        "        raise FileNotFoundError(f\"AOI file not found: {path}\")\n",
        "    target = CRS.from_epsg(int(target_epsg))\n",
        "    if HAS_FIONA:\n",
        "        with fiona.open(path) as src:\n",
        "            bounds = src.bounds  # minx, miny, maxx, maxy\n",
        "            if not src.crs:\n",
        "                raise ValueError(\"AOI CRS is undefined.\")\n",
        "            source = CRS(src.crs)\n",
        "            if source == target:\n",
        "                return bounds\n",
        "            transformer = Transformer.from_crs(source, target, always_xy=True)\n",
        "            minx, miny = transformer.transform(bounds[0], bounds[1])\n",
        "            maxx, maxy = transformer.transform(bounds[2], bounds[3])\n",
        "            return (min(minx, maxx), min(miny, maxy), max(minx, maxx), max(miny, maxy))\n",
        "    if HAS_GDAL_PY:\n",
        "        datasource = gdal.OpenEx(str(path), gdal.OF_VECTOR)\n",
        "        if datasource is None:\n",
        "            raise RuntimeError(\"Unable to read AOI with GDAL.\")\n",
        "        layer = datasource.GetLayer(0)\n",
        "        extent = layer.GetExtent()  # minx, maxx, miny, maxy\n",
        "        spatial_ref = layer.GetSpatialRef()\n",
        "        datasource = None\n",
        "        if spatial_ref is None:\n",
        "            raise ValueError(\"AOI CRS is undefined.\")\n",
        "        source = CRS.from_wkt(spatial_ref.ExportToWkt())\n",
        "        if source == target:\n",
        "            return (extent[0], extent[2], extent[1], extent[3])\n",
        "        transformer = Transformer.from_crs(source, target, always_xy=True)\n",
        "        minx, miny = transformer.transform(extent[0], extent[2])\n",
        "        maxx, maxy = transformer.transform(extent[1], extent[3])\n",
        "        return (min(minx, maxx), min(miny, maxy), max(minx, maxx), max(miny, maxy))\n",
        "    raise RuntimeError(\"Reading an AOI requires either Fiona or GDAL Python bindings.\")\n",
        "\n",
        "\n",
        "def derive_extent(\n",
        "    df: pd.DataFrame,\n",
        "    cell_size: float,\n",
        "    aoi_path: Optional[str],\n",
        "    target_epsg: int,\n",
        ") -> Tuple[float, float, float, float]:\n",
        "    extent = None\n",
        "    if aoi_path:\n",
        "        try:\n",
        "            extent = read_aoi_bounds(aoi_path, target_epsg)\n",
        "            if extent:\n",
        "                rounded = [round(val, 2) for val in extent]\n",
        "                print(f\"Using AOI-derived extent: {rounded}\")\n",
        "        except Exception as exc:\n",
        "            print(f\"AOI extent unavailable ({exc}); falling back to point coverage.\")\n",
        "    if extent is None:\n",
        "        xmin = float(df[\"x\"].min())\n",
        "        xmax = float(df[\"x\"].max())\n",
        "        ymin = float(df[\"y\"].min())\n",
        "        ymax = float(df[\"y\"].max())\n",
        "        buffer_size = max(cell_size, cell_size * 0.5)\n",
        "        extent = (xmin - buffer_size, ymin - buffer_size, xmax + buffer_size, ymax + buffer_size)\n",
        "        print(\"Point-derived extent (with buffer) will be used.\")\n",
        "    if extent[0] >= extent[2] or extent[1] >= extent[3]:\n",
        "        raise ValueError(\"Extent is invalid; please verify inputs.\")\n",
        "    return extent\n",
        "\n",
        "\n",
        "def build_grid(extent: Tuple[float, float, float, float], cell_size: float) -> Dict[str, np.ndarray]:\n",
        "    xmin, ymin, xmax, ymax = extent\n",
        "    width = max(1, int(math.ceil((xmax - xmin) / cell_size)))\n",
        "    height = max(1, int(math.ceil((ymax - ymin) / cell_size)))\n",
        "    xmax_adjusted = xmin + width * cell_size\n",
        "    ymax_adjusted = ymin + height * cell_size\n",
        "    transform = (xmin, cell_size, 0.0, ymax_adjusted, 0.0, -cell_size)\n",
        "    x_centers = xmin + cell_size * (np.arange(width) + 0.5)\n",
        "    y_centers = ymax_adjusted - cell_size * (np.arange(height) + 0.5)\n",
        "    grid_x, grid_y = np.meshgrid(x_centers, y_centers)\n",
        "    return {\n",
        "        \"width\": width,\n",
        "        \"height\": height,\n",
        "        \"transform\": transform,\n",
        "        \"grid_x\": grid_x,\n",
        "        \"grid_y\": grid_y,\n",
        "        \"x_coords\": x_centers,\n",
        "        \"y_coords\": y_centers,\n",
        "    }\n",
        "\n",
        "\n",
        "def create_vrt(csv_path: Path, layer_name: str, epsg: int, x_field: str = \"x\", y_field: str = \"y\") -> str:\n",
        "    sanitized = Path(csv_path).as_posix()\n",
        "    return f'''<OGRVRTDataSource>\n",
        "  <OGRVRTLayer name=\"{layer_name}\">\n",
        "    <SrcDataSource>{sanitized}</SrcDataSource>\n",
        "    <GeometryType>wkbPoint</GeometryType>\n",
        "    <LayerSRS>EPSG:{epsg}</LayerSRS>\n",
        "    <GeometryField encoding=\"PointFromColumns\" x=\"{x_field}\" y=\"{y_field}\"/>\n",
        "  </OGRVRTLayer>\n",
        "</OGRVRTDataSource>\n",
        "'''\n",
        "\n",
        "\n",
        "def run_command(command: Sequence[str], env: Optional[Dict[str, str]] = None) -> None:\n",
        "    process = subprocess.run(command, capture_output=True, text=True, env=env)\n",
        "    if process.stdout:\n",
        "        print(process.stdout.strip())\n",
        "    if process.returncode != 0:\n",
        "        stderr = process.stderr.strip()\n",
        "        raise RuntimeError(f\"Command failed ({command[0]}): {stderr or process.stdout}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "50962be4",
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_band_descriptions(raster_path: Path, band_names: Sequence[str]) -> None:\n",
        "    if not band_names:\n",
        "        return\n",
        "    raster_path = Path(raster_path)\n",
        "    applied = False\n",
        "    if HAS_GDAL_PY:\n",
        "        dataset = gdal.Open(str(raster_path), gdal.GA_Update)\n",
        "        if dataset is not None:\n",
        "            for index, name in enumerate(band_names, start=1):\n",
        "                band = dataset.GetRasterBand(index)\n",
        "                if band is not None:\n",
        "                    band.SetDescription(str(name))\n",
        "            dataset.FlushCache()\n",
        "            dataset = None\n",
        "            applied = True\n",
        "    if not applied and HAS_RASTERIO:\n",
        "        try:\n",
        "            with rasterio.open(raster_path, \"r+\") as dst:\n",
        "                for index, name in enumerate(band_names, start=1):\n",
        "                    dst.set_band_description(index, str(name))\n",
        "            applied = True\n",
        "        except Exception:\n",
        "            pass\n",
        "    if not applied:\n",
        "        print(f\"Warning: unable to set band names for {raster_path}\")\n",
        "\n",
        "\n",
        "def write_geotiff_stack(\n",
        "    stack: np.ndarray,\n",
        "    transform: Tuple[float, float, float, float, float, float],\n",
        "    epsg: int,\n",
        "    output_path: Path,\n",
        "    band_names: Optional[Sequence[str]] = None,\n",
        "    nodata: float = NODATA_VALUE,\n",
        ") -> None:\n",
        "    output_path = Path(output_path)\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    bands, height, width = stack.shape\n",
        "    band_names = list(band_names or [])\n",
        "    if HAS_GDAL_PY:\n",
        "        driver = gdal.GetDriverByName(\"GTiff\")\n",
        "        dataset = driver.Create(\n",
        "            str(output_path),\n",
        "            width,\n",
        "            height,\n",
        "            bands,\n",
        "            gdal.GDT_Float32,\n",
        "            options=[\"COMPRESS=DEFLATE\", \"TILED=YES\", \"BIGTIFF=IF_SAFER\"],\n",
        "        )\n",
        "        dataset.SetGeoTransform(transform)\n",
        "        srs = osr.SpatialReference()\n",
        "        srs.ImportFromEPSG(int(epsg))\n",
        "        dataset.SetProjection(srs.ExportToWkt())\n",
        "        for index in range(bands):\n",
        "            band = dataset.GetRasterBand(index + 1)\n",
        "            data = stack[index].astype(np.float32)\n",
        "            mask = ~np.isfinite(data)\n",
        "            if mask.any():\n",
        "                data = data.copy()\n",
        "                data[mask] = nodata\n",
        "            band.WriteArray(data)\n",
        "            band.SetNoDataValue(nodata)\n",
        "            if index < len(band_names):\n",
        "                band.SetDescription(str(band_names[index]))\n",
        "        dataset.FlushCache()\n",
        "        dataset = None\n",
        "        return\n",
        "    if HAS_RASTERIO and Affine is not None:\n",
        "        affine = Affine(transform[1], transform[2], transform[0], transform[4], transform[5], transform[3])\n",
        "        with rasterio.open(\n",
        "            output_path,\n",
        "            \"w\",\n",
        "            driver=\"GTiff\",\n",
        "            height=height,\n",
        "            width=width,\n",
        "            count=bands,\n",
        "            dtype=\"float32\",\n",
        "            crs=f\"EPSG:{epsg}\",\n",
        "            transform=affine,\n",
        "            nodata=nodata,\n",
        "            compress=\"DEFLATE\",\n",
        "            tiled=True,\n",
        "            BIGTIFF=\"IF_SAFER\",\n",
        "        ) as dst:\n",
        "            for index in range(bands):\n",
        "                data = stack[index].astype(np.float32)\n",
        "                mask = ~np.isfinite(data)\n",
        "                if mask.any():\n",
        "                    data = data.copy()\n",
        "                    data[mask] = nodata\n",
        "                dst.write(data, index + 1)\n",
        "                if index < len(band_names):\n",
        "                    dst.set_band_description(index + 1, str(band_names[index]))\n",
        "        return\n",
        "    raise RuntimeError(\"Writing GeoTIFF rasters requires GDAL or rasterio.\")\n",
        "\n",
        "\n",
        "def idw_interpolate(\n",
        "    coords: np.ndarray,\n",
        "    values: np.ndarray,\n",
        "    grid_x: np.ndarray,\n",
        "    grid_y: np.ndarray,\n",
        "    power: float,\n",
        "    workers: int,\n",
        ") -> np.ndarray:\n",
        "    if coords.size == 0:\n",
        "        return np.full_like(grid_x, np.nan, dtype=np.float32)\n",
        "    tree = cKDTree(coords)\n",
        "    samples = np.column_stack([grid_x.ravel(), grid_y.ravel()])\n",
        "    neighbors = min(coords.shape[0], 12)\n",
        "    distances, indices = tree.query(samples, k=neighbors, workers=max(1, workers))\n",
        "    if neighbors == 1:\n",
        "        distances = distances[:, np.newaxis]\n",
        "        indices = indices[:, np.newaxis]\n",
        "    zero_mask = distances == 0\n",
        "    results = np.empty(samples.shape[0], dtype=np.float32)\n",
        "    if zero_mask.any():\n",
        "        zero_rows = zero_mask.any(axis=1)\n",
        "        zero_indices = zero_mask[zero_rows].argmax(axis=1)\n",
        "        results[zero_rows] = values[indices[zero_rows, zero_indices]]\n",
        "    else:\n",
        "        zero_rows = np.zeros(samples.shape[0], dtype=bool)\n",
        "    remaining = ~zero_rows\n",
        "    if remaining.any():\n",
        "        weights = np.where(distances[remaining] == 0, 0.0, 1.0 / np.power(distances[remaining], power))\n",
        "        weight_sum = weights.sum(axis=1)\n",
        "        estimates = np.full(weight_sum.shape, np.nan, dtype=np.float32)\n",
        "        valid = weight_sum > 0\n",
        "        if valid.any():\n",
        "            weighted = weights[valid] * values[indices[remaining][valid]]\n",
        "            estimates[valid] = weighted.sum(axis=1) / weight_sum[valid]\n",
        "        results[remaining] = estimates\n",
        "    return results.reshape(grid_x.shape)\n",
        "\n",
        "\n",
        "def create_master_stack(band_plan: Sequence[Dict[str, str]], output_path: Path) -> Optional[Path]:\n",
        "    band_plan = list(band_plan)\n",
        "    if not band_plan:\n",
        "        return None\n",
        "    output_path = Path(output_path)\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    template_path = Path(band_plan[0][\"path\"])\n",
        "    if HAS_GDAL_PY:\n",
        "        template = gdal.Open(str(template_path))\n",
        "        if template is None:\n",
        "            raise RuntimeError(f\"Unable to open template raster: {template_path}\")\n",
        "        width = template.RasterXSize\n",
        "        height = template.RasterYSize\n",
        "        transform = template.GetGeoTransform()\n",
        "        projection = template.GetProjection()\n",
        "        driver = gdal.GetDriverByName(\"GTiff\")\n",
        "        dataset = driver.Create(\n",
        "            str(output_path),\n",
        "            width,\n",
        "            height,\n",
        "            len(band_plan),\n",
        "            gdal.GDT_Float32,\n",
        "            options=[\"COMPRESS=DEFLATE\", \"TILED=YES\", \"BIGTIFF=IF_SAFER\"],\n",
        "        )\n",
        "        dataset.SetGeoTransform(transform)\n",
        "        if projection:\n",
        "            dataset.SetProjection(projection)\n",
        "        for index, plan in enumerate(band_plan, start=1):\n",
        "            source = gdal.Open(str(plan[\"path\"]))\n",
        "            if source is None:\n",
        "                raise RuntimeError(f\"Unable to open {plan['path']} for master stack.\")\n",
        "            src_band = source.GetRasterBand(int(plan[\"band\"]))\n",
        "            data = src_band.ReadAsArray().astype(np.float32)\n",
        "            mask = ~np.isfinite(data)\n",
        "            if mask.any():\n",
        "                data = data.copy()\n",
        "                data[mask] = NODATA_VALUE\n",
        "            dst_band = dataset.GetRasterBand(index)\n",
        "            dst_band.WriteArray(data)\n",
        "            dst_band.SetNoDataValue(NODATA_VALUE)\n",
        "            dst_band.SetDescription(plan.get(\"label\", \"\"))\n",
        "            source = None\n",
        "        dataset.FlushCache()\n",
        "        dataset = None\n",
        "        template = None\n",
        "        return output_path\n",
        "    if HAS_RASTERIO:\n",
        "        with rasterio.open(template_path) as template:\n",
        "            profile = template.profile\n",
        "        profile.update(\n",
        "            count=len(band_plan),\n",
        "            dtype=\"float32\",\n",
        "            compress=\"DEFLATE\",\n",
        "            tiled=True,\n",
        "            BIGTIFF=\"IF_SAFER\",\n",
        "            nodata=NODATA_VALUE,\n",
        "        )\n",
        "        with rasterio.open(output_path, \"w\", **profile) as dst:\n",
        "            for index, plan in enumerate(band_plan, start=1):\n",
        "                with rasterio.open(plan[\"path\"]) as src:\n",
        "                    data = src.read(int(plan[\"band\"])).astype(np.float32)\n",
        "                mask = ~np.isfinite(data)\n",
        "                if mask.any():\n",
        "                    data = data.copy()\n",
        "                    data[mask] = NODATA_VALUE\n",
        "                dst.write(data, index)\n",
        "                dst.set_band_description(index, plan.get(\"label\", \"\"))\n",
        "        return output_path\n",
        "    if GDAL_AVAILABLE:\n",
        "        with tempfile.TemporaryDirectory() as tmpdir:\n",
        "            tmpdir = Path(tmpdir)\n",
        "            vrt_path = tmpdir / \"master_stack.vrt\"\n",
        "            ordered_files: List[str] = []\n",
        "            for entry in band_plan:\n",
        "                path = entry[\"path\"]\n",
        "                if not ordered_files or ordered_files[-1] != path:\n",
        "                    ordered_files.append(path)\n",
        "            command = [GDAL_CMDS[\"gdalbuildvrt\"], \"-separate\", str(vrt_path), *ordered_files]\n",
        "            run_command(command)\n",
        "            translate_command = [\n",
        "                GDAL_CMDS[\"gdal_translate\"],\n",
        "                str(vrt_path),\n",
        "                str(output_path),\n",
        "                \"-co\",\n",
        "                \"COMPRESS=DEFLATE\",\n",
        "                \"-co\",\n",
        "                \"TILED=YES\",\n",
        "                \"-co\",\n",
        "                \"BIGTIFF=IF_SAFER\",\n",
        "                \"-a_nodata\",\n",
        "                str(NODATA_VALUE),\n",
        "            ]\n",
        "            run_command(translate_command)\n",
        "        return output_path\n",
        "    raise RuntimeError(\"Unable to create master GeoTIFF; install GDAL (CLI/Python) or rasterio.\")\n",
        "\n",
        "\n",
        "def read_raster_stack(raster_path: Path) -> Tuple[np.ndarray, Tuple[float, float, float, float, float, float], int, int]:\n",
        "    raster_path = Path(raster_path)\n",
        "    if HAS_GDAL_PY:\n",
        "        dataset = gdal.Open(str(raster_path))\n",
        "        if dataset is None:\n",
        "            raise RuntimeError(f\"Unable to open raster: {raster_path}\")\n",
        "        width = dataset.RasterXSize\n",
        "        height = dataset.RasterYSize\n",
        "        transform = dataset.GetGeoTransform()\n",
        "        data = []\n",
        "        for band_index in range(dataset.RasterCount):\n",
        "            band = dataset.GetRasterBand(band_index + 1)\n",
        "            data.append(band.ReadAsArray().astype(np.float32))\n",
        "        dataset = None\n",
        "        return np.stack(data), transform, width, height\n",
        "    if HAS_RASTERIO:\n",
        "        with rasterio.open(raster_path) as src:\n",
        "            data = src.read().astype(np.float32)\n",
        "            transform = src.transform.to_gdal()\n",
        "            width = src.width\n",
        "            height = src.height\n",
        "        return data, transform, width, height\n",
        "    raise RuntimeError(\"Reading rasters requires GDAL or rasterio.\")\n",
        "\n",
        "\n",
        "def build_timeseries_netcdf(\n",
        "    time_plan: Sequence[Dict[str, object]],\n",
        "    variables: Sequence[str],\n",
        "    epsg: int,\n",
        "    output_path: Path,\n",
        ") -> None:\n",
        "    if not time_plan:\n",
        "        return\n",
        "    if not HAS_XARRAY:\n",
        "        print(\"Skipping NetCDF export because xarray is unavailable.\")\n",
        "        return\n",
        "    output_path = Path(output_path)\n",
        "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    _, transform, width, height = read_raster_stack(time_plan[0][\"path\"])\n",
        "    x_coords = transform[0] + transform[1] * (np.arange(width) + 0.5)\n",
        "    y_coords = transform[3] + transform[5] * (np.arange(height) + 0.5)\n",
        "    data_store: Dict[str, List[np.ndarray]] = {var: [] for var in variables}\n",
        "    times: List[pd.Timestamp] = []\n",
        "    for entry in time_plan:\n",
        "        stack, _, _, _ = read_raster_stack(entry[\"path\"])\n",
        "        if stack.shape[0] < len(variables):\n",
        "            raise ValueError(f\"Raster {entry['path']} has fewer bands than expected.\")\n",
        "        for idx, variable in enumerate(variables):\n",
        "            data_store[variable].append(stack[idx])\n",
        "        times.append(pd.to_datetime(entry[\"date\"]))\n",
        "    coords = {\n",
        "        \"time\": pd.to_datetime(times),\n",
        "        \"y\": y_coords,\n",
        "        \"x\": x_coords,\n",
        "    }\n",
        "    data_vars = {\n",
        "        variable: ((\"time\", \"y\", \"x\"), np.stack(arrays, axis=0))\n",
        "        for variable, arrays in data_store.items()\n",
        "    }\n",
        "    dataset = xr.Dataset(data_vars=data_vars, coords=coords, attrs={\"crs\": f\"EPSG:{epsg}\", \"nodata\": NODATA_VALUE})\n",
        "    dataset.to_netcdf(output_path)\n",
        "    dataset.close()\n",
        "    print(f\"NetCDF time series created: {output_path}\")\n",
        "\n",
        "\n",
        "def interpolate_with_gdal(\n",
        "    group_df: pd.DataFrame,\n",
        "    variables: Sequence[str],\n",
        "    epsg: int,\n",
        "    extent: Tuple[float, float, float, float],\n",
        "    cell_size: float,\n",
        "    power: float,\n",
        "    jobs: int,\n",
        "    output_path: Path,\n",
        ") -> None:\n",
        "    if not GDAL_AVAILABLE:\n",
        "        raise RuntimeError(\"GDAL CLI tools are not available.\")\n",
        "    grid = build_grid(extent, cell_size)\n",
        "    env = os.environ.copy()\n",
        "    env[\"GDAL_NUM_THREADS\"] = str(max(1, int(jobs)))\n",
        "    with tempfile.TemporaryDirectory() as tmpdir:\n",
        "        tmpdir = Path(tmpdir)\n",
        "        subset_path = tmpdir / \"points.csv\"\n",
        "        export_cols = [\"x\", \"y\", *variables]\n",
        "        group_df[export_cols].to_csv(subset_path, index=False)\n",
        "        vrt_path = tmpdir / \"points.vrt\"\n",
        "        vrt_path.write_text(create_vrt(subset_path, \"points\", epsg, \"x\", \"y\"), encoding=\"utf-8\")\n",
        "        per_band_files: List[str] = []\n",
        "        for variable in tqdm(variables, desc=\"Variables (GDAL)\", leave=False):\n",
        "            band_path = tmpdir / f\"{variable}.tif\"\n",
        "            command = [\n",
        "                GDAL_CMDS[\"gdal_grid\"],\n",
        "                \"-zfield\",\n",
        "                variable,\n",
        "                \"-a\",\n",
        "                f\"invdist:power={power}\",\n",
        "                \"-txe\",\n",
        "                str(extent[0]),\n",
        "                str(extent[2]),\n",
        "                \"-tye\",\n",
        "                str(extent[1]),\n",
        "                str(extent[3]),\n",
        "                \"-outsize\",\n",
        "                str(grid[\"width\"]),\n",
        "                str(grid[\"height\"]),\n",
        "                \"-a_srs\",\n",
        "                f\"EPSG:{epsg}\",\n",
        "                \"-ot\",\n",
        "                \"Float32\",\n",
        "                \"-of\",\n",
        "                \"GTiff\",\n",
        "                \"-l\",\n",
        "                \"points\",\n",
        "                str(vrt_path),\n",
        "                str(band_path),\n",
        "            ]\n",
        "            run_command(command, env=env)\n",
        "            per_band_files.append(str(band_path))\n",
        "        stack_vrt = tmpdir / \"stack.vrt\"\n",
        "        run_command(\n",
        "            [GDAL_CMDS[\"gdalbuildvrt\"], \"-separate\", str(stack_vrt), *per_band_files],\n",
        "            env=env,\n",
        "        )\n",
        "        translate_command = [\n",
        "            GDAL_CMDS[\"gdal_translate\"],\n",
        "            str(stack_vrt),\n",
        "            str(output_path),\n",
        "            \"-co\",\n",
        "            \"COMPRESS=DEFLATE\",\n",
        "            \"-co\",\n",
        "            \"TILED=YES\",\n",
        "            \"-co\",\n",
        "            \"BIGTIFF=IF_SAFER\",\n",
        "            \"-a_nodata\",\n",
        "            str(NODATA_VALUE),\n",
        "        ]\n",
        "        run_command(translate_command, env=env)\n",
        "    set_band_descriptions(output_path, variables)\n",
        "\n",
        "\n",
        "def interpolate_with_python(\n",
        "    group_df: pd.DataFrame,\n",
        "    variables: Sequence[str],\n",
        "    epsg: int,\n",
        "    extent: Tuple[float, float, float, float],\n",
        "    cell_size: float,\n",
        "    power: float,\n",
        "    jobs: int,\n",
        "    output_path: Path,\n",
        ") -> None:\n",
        "    if not HAS_SCIPY:\n",
        "        raise RuntimeError(\"SciPy is required for python_idw_kdtree interpolation.\")\n",
        "    grid = build_grid(extent, cell_size)\n",
        "    coords = group_df[[\"x\", \"y\"]].to_numpy()\n",
        "    stack = []\n",
        "    for variable in tqdm(variables, desc=\"Variables (Python)\", leave=False):\n",
        "        values = pd.to_numeric(group_df[variable], errors=\"coerce\").to_numpy(dtype=np.float32)\n",
        "        mask = np.isfinite(values)\n",
        "        if not mask.any():\n",
        "            stack.append(np.full((grid[\"height\"], grid[\"width\"]), np.nan, dtype=np.float32))\n",
        "            continue\n",
        "        surface = idw_interpolate(\n",
        "            coords[mask],\n",
        "            values[mask],\n",
        "            grid[\"grid_x\"],\n",
        "            grid[\"grid_y\"],\n",
        "            power,\n",
        "            max(1, int(jobs)),\n",
        "        )\n",
        "        stack.append(surface.astype(np.float32))\n",
        "    stack_array = np.stack(stack)\n",
        "    write_geotiff_stack(stack_array, grid[\"transform\"], epsg, output_path, band_names=list(variables))\n",
        "\n",
        "\n",
        "def interpolate_with_pykrige(\n",
        "    group_df: pd.DataFrame,\n",
        "    variables: Sequence[str],\n",
        "    epsg: int,\n",
        "    extent: Tuple[float, float, float, float],\n",
        "    cell_size: float,\n",
        "    jobs: int,\n",
        "    output_path: Path,\n",
        ") -> None:\n",
        "    if not HAS_PYKRIGE:\n",
        "        raise RuntimeError(\"PyKrige is required for pykrige_ok interpolation.\")\n",
        "    grid = build_grid(extent, cell_size)\n",
        "    x_coords = grid[\"x_coords\"]\n",
        "    y_coords = grid[\"y_coords\"]\n",
        "    stack = []\n",
        "    for variable in tqdm(variables, desc=\"Variables (PyKrige)\", leave=False):\n",
        "        values = pd.to_numeric(group_df[variable], errors=\"coerce\").to_numpy(dtype=np.float32)\n",
        "        mask = np.isfinite(values)\n",
        "        if mask.sum() < 3:\n",
        "            stack.append(np.full((grid[\"height\"], grid[\"width\"]), np.nan, dtype=np.float32))\n",
        "            continue\n",
        "        try:\n",
        "            ok = OrdinaryKriging(\n",
        "                group_df.loc[mask, \"x\"].to_numpy(),\n",
        "                group_df.loc[mask, \"y\"].to_numpy(),\n",
        "                values[mask],\n",
        "                variogram_model=\"spherical\",\n",
        "                verbose=False,\n",
        "                enable_plotting=False,\n",
        "                coordinates_type=\"euclidean\",\n",
        "            )\n",
        "            surface, _ = ok.execute(\"grid\", x_coords, y_coords, backend=\"vectorized\")\n",
        "            stack.append(np.asarray(surface, dtype=np.float32))\n",
        "        except Exception as exc:\n",
        "            print(f\"Kriging failed for {variable}: {exc}\")\n",
        "            stack.append(np.full((grid[\"height\"], grid[\"width\"]), np.nan, dtype=np.float32))\n",
        "    stack_array = np.stack(stack)\n",
        "    write_geotiff_stack(stack_array, grid[\"transform\"], epsg, output_path, band_names=list(variables))\n",
        "\n",
        "\n",
        "def resolve_method(preferred: str) -> str:\n",
        "    if preferred == \"gdal_grid\" and not GDAL_AVAILABLE:\n",
        "        print(\"GDAL CLI tools were not detected; switching to python_idw_kdtree.\")\n",
        "        return \"python_idw_kdtree\"\n",
        "    if preferred == \"python_idw_kdtree\" and not HAS_SCIPY:\n",
        "        raise RuntimeError(\"SciPy is required for python_idw_kdtree.\")\n",
        "    if preferred == \"pykrige_ok\" and not HAS_PYKRIGE:\n",
        "        raise RuntimeError(\"PyKrige is required for pykrige_ok.\")\n",
        "    return preferred\n",
        "\n",
        "\n",
        "def process_workflow(\n",
        "    csv_path: str,\n",
        "    aoi_path: Optional[str],\n",
        "    lon_col: str,\n",
        "    lat_col: str,\n",
        "    date_col: str,\n",
        "    variable_cols: Sequence[str],\n",
        "    method: str,\n",
        "    cell_size: float,\n",
        "    power: float,\n",
        "    jobs: int,\n",
        "    output_folder: str,\n",
        ") -> None:\n",
        "    if cell_size <= 0:\n",
        "        raise ValueError(\"Cell size must be greater than zero.\")\n",
        "    if power <= 0:\n",
        "        raise ValueError(\"IDW power must be positive.\")\n",
        "    if jobs <= 0:\n",
        "        raise ValueError(\"Parallel jobs must be at least 1.\")\n",
        "    csv_path = Path(csv_path).expanduser()\n",
        "    if not csv_path.exists():\n",
        "        raise FileNotFoundError(f\"CSV file not found: {csv_path}\")\n",
        "    output_dir = Path(output_folder).expanduser()\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"Reading CSV: {csv_path}\")\n",
        "    df = pd.read_csv(csv_path)\n",
        "    prepared = prepare_dataframe(df, lon_col, lat_col, date_col, variable_cols)\n",
        "    lon_mean = float(prepared[lon_col].mean())\n",
        "    lat_mean = float(prepared[lat_col].mean())\n",
        "    target_epsg = determine_utm_epsg(lon_mean, lat_mean)\n",
        "    print(f\"Target CRS: EPSG:{target_epsg}\")\n",
        "    projected = reproject_dataframe(prepared, lon_col, lat_col, target_epsg)\n",
        "    extent = derive_extent(projected, cell_size, aoi_path, target_epsg)\n",
        "    method_to_use = resolve_method(method)\n",
        "    grouped = list(projected.groupby(\"__cheaqi_date\", sort=True))\n",
        "    if not grouped:\n",
        "        raise ValueError(\"No dated observations were found after filtering.\")\n",
        "    band_records = []\n",
        "    master_plan = []\n",
        "    time_series_plan = []\n",
        "    print(f\"Processing {len(grouped)} date group(s) with method '{method_to_use}'.\")\n",
        "    for date_value, group in tqdm(grouped, desc=\"Dates\"):\n",
        "        label = date_value.strftime(\"%Y%m%d\")\n",
        "        output_path = output_dir / f\"CHEAQI_{label}.tif\"\n",
        "        print(f\"Interpolating {label} -> {output_path.name}\")\n",
        "        if method_to_use == \"gdal_grid\":\n",
        "            interpolate_with_gdal(group, variable_cols, target_epsg, extent, cell_size, power, jobs, output_path)\n",
        "        elif method_to_use == \"python_idw_kdtree\":\n",
        "            interpolate_with_python(group, variable_cols, target_epsg, extent, cell_size, power, jobs, output_path)\n",
        "        elif method_to_use == \"pykrige_ok\":\n",
        "            interpolate_with_pykrige(group, variable_cols, target_epsg, extent, cell_size, jobs, output_path)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported method: {method_to_use}\")\n",
        "        time_series_plan.append({\"date\": date_value, \"path\": str(output_path)})\n",
        "        for band_index, variable in enumerate(variable_cols, start=1):\n",
        "            master_plan.append(\n",
        "                {\n",
        "                    \"path\": str(output_path),\n",
        "                    \"band\": band_index,\n",
        "                    \"label\": f\"{label}_{variable}\",\n",
        "                }\n",
        "            )\n",
        "            band_records.append(\n",
        "                {\n",
        "                    \"raster\": output_path.name,\n",
        "                    \"band\": band_index,\n",
        "                    \"date\": date_value.strftime(\"%Y-%m-%d\"),\n",
        "                    \"variable\": variable,\n",
        "                    \"master_band\": len(master_plan),\n",
        "                    \"master_label\": f\"{label}_{variable}\",\n",
        "                }\n",
        "            )\n",
        "    band_map_path = output_dir / \"CHEAQI_per_date_bandmap.csv\"\n",
        "    pd.DataFrame(band_records).to_csv(band_map_path, index=False)\n",
        "    print(f\"Band map written to: {band_map_path}\")\n",
        "    print(\"Building master GeoTIFF stack...\")\n",
        "    master_output = output_dir / MASTER_STACK_NAME\n",
        "    try:\n",
        "        created = create_master_stack(master_plan, master_output)\n",
        "        if created:\n",
        "            set_band_descriptions(master_output, [entry[\"label\"] for entry in master_plan])\n",
        "            print(f\"Master stack created: {master_output} ({len(master_plan)} band(s)).\")\n",
        "        else:\n",
        "            print(\"No rasters available to build a master stack.\")\n",
        "    except Exception as exc:\n",
        "        print(f\"Master stack creation failed: {exc}\")\n",
        "    netcdf_path = output_dir / NETCDF_TIMESERIES_NAME\n",
        "    try:\n",
        "        build_timeseries_netcdf(time_series_plan, variable_cols, target_epsg, netcdf_path)\n",
        "    except Exception as exc:\n",
        "        print(f\"NetCDF export failed: {exc}\")\n",
        "    print(f\"Interpolation complete. Outputs saved to: {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e075678",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e735205d0eb944128ee262ec8dd527d4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(VBox(children=(HBox(children=(Text(value='', description='CSV Path', layout=Layout(width='70%')‚Ä¶"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Enhanced Interactive CHEAQI Interface with Fixed Action Buttons\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Enhanced Output Area with better styling\n",
        "output_area = widgets.Output(\n",
        "    layout=widgets.Layout(\n",
        "        border=\"2px solid #4CAF50\", \n",
        "        max_height=\"400px\", \n",
        "        overflow=\"auto\",\n",
        "        padding=\"10px\",\n",
        "        background_color=\"#f8f9fa\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Status indicator\n",
        "status_widget = widgets.HTML(\n",
        "    value=\"<div style='padding: 10px; background: #e3f2fd; border-radius: 5px; margin: 10px 0;'>\"\n",
        "          \"<b>üöÄ CHEAQI Interactive Workbench</b><br>\"\n",
        "          \"Select your CSV file and configure interpolation parameters below.</div>\"\n",
        ")\n",
        "\n",
        "# File selection widgets with improved styling\n",
        "csv_path_widget = widgets.Text(\n",
        "    description=\"üìÅ CSV File:\",\n",
        "    placeholder=\"Select your environmental data CSV file...\",\n",
        "    layout=widgets.Layout(width=\"60%\"),\n",
        "    style={'description_width': '120px'}\n",
        ")\n",
        "csv_browse_button = widgets.Button(\n",
        "    description=\"Browse Files\", \n",
        "    icon=\"folder-open\",\n",
        "    button_style=\"info\",\n",
        "    layout=widgets.Layout(width=\"140px\")\n",
        ")\n",
        "\n",
        "aoi_path_widget = widgets.Text(\n",
        "    description=\"üó∫Ô∏è AOI (Optional):\",\n",
        "    placeholder=\"Optional: Area of Interest shapefile...\",\n",
        "    layout=widgets.Layout(width=\"60%\"),\n",
        "    style={'description_width': '120px'}\n",
        ")\n",
        "aoi_browse_button = widgets.Button(\n",
        "    description=\"Browse AOI\", \n",
        "    icon=\"map\",\n",
        "    button_style=\"\",\n",
        "    layout=widgets.Layout(width=\"140px\")\n",
        ")\n",
        "\n",
        "# Column selection dropdowns with better labels\n",
        "lon_dropdown = widgets.Dropdown(\n",
        "    description=\"üåç Longitude:\",\n",
        "    options=[],\n",
        "    disabled=True,\n",
        "    layout=widgets.Layout(width=\"32%\"),\n",
        "    style={'description_width': '100px'}\n",
        ")\n",
        "lat_dropdown = widgets.Dropdown(\n",
        "    description=\"üåç Latitude:\",\n",
        "    options=[],\n",
        "    disabled=True,\n",
        "    layout=widgets.Layout(width=\"32%\"),\n",
        "    style={'description_width': '100px'}\n",
        ")\n",
        "date_dropdown = widgets.Dropdown(\n",
        "    description=\"üìÖ Date Column:\",\n",
        "    options=[],\n",
        "    disabled=True,\n",
        "    layout=widgets.Layout(width=\"32%\"),\n",
        "    style={'description_width': '100px'}\n",
        ")\n",
        "\n",
        "# Enhanced variable selector\n",
        "variable_select = widgets.SelectMultiple(\n",
        "    description=\"üìä Variables:\",\n",
        "    options=[],\n",
        "    disabled=True,\n",
        "    rows=8,\n",
        "    layout=widgets.Layout(width=\"50%\", height=\"200px\"),\n",
        "    style={'description_width': '100px'}\n",
        ")\n",
        "\n",
        "# Variable preview widget\n",
        "variable_preview = widgets.HTML(\n",
        "    value=\"<div style='padding: 10px; background: #f5f5f5; border-radius: 5px;'>\"\n",
        "          \"<b>Variable Preview:</b><br>Load CSV to see available variables.</div>\",\n",
        "    layout=widgets.Layout(width=\"48%\", height=\"200px\")\n",
        ")\n",
        "\n",
        "# Method and parameter controls with enhanced styling  \n",
        "method_dropdown = widgets.Dropdown(\n",
        "    description=\"‚öôÔ∏è Method:\",\n",
        "    options=[\n",
        "        (\"üîß GDAL IDW (Recommended)\", \"gdal_grid\"),\n",
        "        (\"üêç Python KDTree (Fast)\", \"python_idw_kdtree\"),\n",
        "        (\"üìà PyKrige (Statistical)\", \"pykrige_ok\"),\n",
        "    ],\n",
        "    value=\"gdal_grid\",\n",
        "    layout=widgets.Layout(width=\"100%\"),\n",
        "    style={'description_width': '100px'}\n",
        ")\n",
        "\n",
        "# Parameter controls in organized layout\n",
        "cell_size_input = widgets.FloatText(\n",
        "    description=\"üìè Cell Size (m):\",\n",
        "    value=1000.0,\n",
        "    min=1.0,\n",
        "    layout=widgets.Layout(width=\"48%\"),\n",
        "    style={'description_width': '120px'}\n",
        ")\n",
        "power_input = widgets.FloatText(\n",
        "    description=\"‚ö° IDW Power:\",\n",
        "    value=2.0,\n",
        "    min=0.1,\n",
        "    max=10.0,\n",
        "    layout=widgets.Layout(width=\"48%\"),\n",
        "    style={'description_width': '120px'}\n",
        ")\n",
        "\n",
        "cpu_guess = max(1, os.cpu_count() or 4)\n",
        "jobs_input = widgets.BoundedIntText(\n",
        "    description=\"üîÑ CPU Jobs:\",\n",
        "    value=min(4, cpu_guess),\n",
        "    min=1,\n",
        "    max=max(1, cpu_guess * 2),\n",
        "    layout=widgets.Layout(width=\"48%\"),\n",
        "    style={'description_width': '120px'}\n",
        ")\n",
        "\n",
        "# Quality control toggle\n",
        "quality_check = widgets.Checkbox(\n",
        "    description=\"üîç Enable Quality Assessment\",\n",
        "    value=True,\n",
        "    layout=widgets.Layout(width=\"48%\"),\n",
        "    style={'description_width': '200px'}\n",
        ")\n",
        "\n",
        "# Output directory selection\n",
        "output_path_widget = widgets.Text(\n",
        "    description=\"üìÇ Output Dir:\",\n",
        "    placeholder=\"Select output directory for results...\",\n",
        "    layout=widgets.Layout(width=\"60%\"),\n",
        "    style={'description_width': '120px'}\n",
        ")\n",
        "output_browse_button = widgets.Button(\n",
        "    description=\"Browse Folder\", \n",
        "    icon=\"folder\",\n",
        "    button_style=\"\",\n",
        "    layout=widgets.Layout(width=\"140px\")\n",
        ")\n",
        "\n",
        "# Action buttons with enhanced styling\n",
        "load_columns_button = widgets.Button(\n",
        "    description=\"üîÑ Load & Analyze CSV\",\n",
        "    button_style=\"info\",\n",
        "    icon=\"refresh\",\n",
        "    layout=widgets.Layout(width=\"200px\", height=\"40px\")\n",
        ")\n",
        "\n",
        "preview_button = widgets.Button(\n",
        "    description=\"üëÄ Preview Selection\",\n",
        "    button_style=\"\",\n",
        "    icon=\"eye\",\n",
        "    layout=widgets.Layout(width=\"200px\", height=\"40px\")\n",
        ")\n",
        "\n",
        "run_button = widgets.Button(\n",
        "    description=\"üöÄ Run Interpolation\",\n",
        "    button_style=\"success\",\n",
        "    icon=\"play\",\n",
        "    layout=widgets.Layout(width=\"200px\", height=\"40px\")\n",
        ")\n",
        "\n",
        "# Progress bar\n",
        "progress_bar = widgets.IntProgress(\n",
        "    value=0,\n",
        "    min=0,\n",
        "    max=100,\n",
        "    description='Progress:',\n",
        "    bar_style='info',\n",
        "    orientation='horizontal',\n",
        "    layout=widgets.Layout(width=\"100%\")\n",
        ")\n",
        "\n",
        "def select_path(target_widget: widgets.Text, select_directory: bool, filetypes=None):\n",
        "    \"\"\"Enhanced file selection with better error handling\"\"\"\n",
        "    try:\n",
        "        import tkinter as tk\n",
        "        from tkinter import filedialog\n",
        "        \n",
        "        root = tk.Tk()\n",
        "        root.withdraw()\n",
        "        root.attributes(\"-topmost\", True)\n",
        "        \n",
        "        if select_directory:\n",
        "            path = filedialog.askdirectory(title=\"Select Output Directory\")\n",
        "        else:\n",
        "            if filetypes is None:\n",
        "                filetypes = [(\"CSV files\", \"*.csv\"), (\"All files\", \"*.*\")]\n",
        "            path = filedialog.askopenfilename(\n",
        "                title=\"Select CSV File\",\n",
        "                filetypes=filetypes\n",
        "            )\n",
        "        \n",
        "        root.destroy()\n",
        "        \n",
        "        if path:\n",
        "            target_widget.value = path\n",
        "            with output_area:\n",
        "                print(f\"‚úÖ Selected: {Path(path).name}\")\n",
        "                \n",
        "    except Exception as exc:\n",
        "        with output_area:\n",
        "            print(f\"‚ùå File dialog error: {exc}\")\n",
        "            print(\"üí° Tip: Manually type the file path in the text field\")\n",
        "\n",
        "def guess_column(columns, keywords):\n",
        "    \"\"\"Improved column guessing with pattern matching\"\"\"\n",
        "    lowered = {col.lower(): col for col in columns}\n",
        "    \n",
        "    # Exact matches first\n",
        "    for keyword in keywords:\n",
        "        if keyword in lowered:\n",
        "            return lowered[keyword]\n",
        "    \n",
        "    # Partial matches\n",
        "    for col in columns:\n",
        "        for keyword in keywords:\n",
        "            if keyword in col.lower():\n",
        "                return col\n",
        "    \n",
        "    return columns[0] if columns else None\n",
        "\n",
        "def analyze_variables(df, columns):\n",
        "    \"\"\"Analyze variables and provide statistics\"\"\"\n",
        "    analysis = {}\n",
        "    \n",
        "    for col in columns:\n",
        "        if col in df.columns:\n",
        "            series = pd.to_numeric(df[col], errors='coerce')\n",
        "            analysis[col] = {\n",
        "                'type': 'numeric' if series.notna().any() else 'text',\n",
        "                'missing': series.isna().sum(),\n",
        "                'count': len(series),\n",
        "                'missing_pct': (series.isna().sum() / len(series) * 100),\n",
        "                'min': series.min() if series.notna().any() else 'N/A',\n",
        "                'max': series.max() if series.notna().any() else 'N/A',\n",
        "                'mean': series.mean() if series.notna().any() else 'N/A'\n",
        "            }\n",
        "    \n",
        "    return analysis\n",
        "\n",
        "def update_variable_preview(analysis):\n",
        "    \"\"\"Update the variable preview with analysis results\"\"\"\n",
        "    if not analysis:\n",
        "        variable_preview.value = (\n",
        "            \"<div style='padding: 10px; background: #f5f5f5; border-radius: 5px;'>\"\n",
        "            \"<b>Variable Preview:</b><br>Load CSV to see available variables.</div>\"\n",
        "        )\n",
        "        return\n",
        "    \n",
        "    html_content = [\n",
        "        \"<div style='padding: 10px; background: #f5f5f5; border-radius: 5px;'>\",\n",
        "        \"<b>üìä Variable Analysis:</b><br><br>\"\n",
        "    ]\n",
        "    \n",
        "    for var, stats in list(analysis.items())[:10]:  # Show first 10\n",
        "        if stats['type'] == 'numeric':\n",
        "            quality = \"üü¢ Excellent\" if stats['missing_pct'] < 5 else \"üü° Good\" if stats['missing_pct'] < 20 else \"üî¥ Poor\"\n",
        "            html_content.append(\n",
        "                f\"<b>{var}:</b><br>\"\n",
        "                f\"&nbsp;&nbsp;Quality: {quality} ({stats['missing_pct']:.1f}% missing)<br>\"\n",
        "                f\"&nbsp;&nbsp;Range: {stats['min']:.2f} to {stats['max']:.2f}<br><br>\"\n",
        "            )\n",
        "    \n",
        "    if len(analysis) > 10:\n",
        "        html_content.append(f\"<i>... and {len(analysis) - 10} more variables</i><br>\")\n",
        "    \n",
        "    html_content.append(\"</div>\")\n",
        "    variable_preview.value = \"\".join(html_content)\n",
        "\n",
        "# Enhanced event handlers with better feedback\n",
        "def browse_csv(_):\n",
        "    select_path(csv_path_widget, False, [(\"CSV files\", \"*.csv\"), (\"All files\", \"*.*\")])\n",
        "\n",
        "def browse_aoi(_):\n",
        "    select_path(aoi_path_widget, False, [(\"Shapefiles\", \"*.shp\"), (\"GeoJSON\", \"*.geojson\"), (\"All files\", \"*.*\")])\n",
        "\n",
        "def browse_output(_):\n",
        "    select_path(output_path_widget, True)\n",
        "\n",
        "def load_columns(_):\n",
        "    \"\"\"Enhanced column loading with comprehensive analysis\"\"\"\n",
        "    csv_path = Path(csv_path_widget.value).expanduser()\n",
        "    \n",
        "    with output_area:\n",
        "        clear_output()\n",
        "        print(\"üîç Analyzing CSV file...\")\n",
        "        \n",
        "        if not csv_path.exists():\n",
        "            print(f\"‚ùå CSV not found: {csv_path}\")\n",
        "            status_widget.value = (\n",
        "                \"<div style='padding: 10px; background: #ffebee; border-radius: 5px; margin: 10px 0;'>\"\n",
        "                \"<b>‚ùå Error:</b> CSV file not found. Please check the file path.</div>\"\n",
        "            )\n",
        "            return\n",
        "        \n",
        "        try:\n",
        "            # Load sample for analysis\n",
        "            print(\"üìñ Reading CSV structure...\")\n",
        "            sample = pd.read_csv(csv_path, nrows=100)  # Read more for better analysis\n",
        "            \n",
        "            if sample.empty:\n",
        "                print(\"‚ùå CSV file is empty\")\n",
        "                return\n",
        "                \n",
        "            columns = list(sample.columns)\n",
        "            print(f\"‚úÖ Found {len(columns)} columns\")\n",
        "            \n",
        "            # Analyze all columns\n",
        "            analysis = analyze_variables(sample, columns)\n",
        "            \n",
        "            # Smart column detection\n",
        "            lon_guess = guess_column(columns, [\"lon\", \"longitude\", \"x\", \"lng\"])\n",
        "            lat_guess = guess_column(columns, [\"lat\", \"latitude\", \"y\"])\n",
        "            date_guess = guess_column(columns, [\"date\", \"time\", \"timestamp\", \"datetime\"])\n",
        "            \n",
        "            print(f\"üéØ Auto-detected columns:\")\n",
        "            print(f\"   Longitude: {lon_guess}\")\n",
        "            print(f\"   Latitude: {lat_guess}\")  \n",
        "            print(f\"   Date: {date_guess}\")\n",
        "            \n",
        "            # Update dropdowns\n",
        "            lon_dropdown.options = columns\n",
        "            lat_dropdown.options = columns\n",
        "            date_dropdown.options = columns\n",
        "            \n",
        "            if lon_guess: lon_dropdown.value = lon_guess\n",
        "            if lat_guess: lat_dropdown.value = lat_guess\n",
        "            if date_guess: date_dropdown.value = date_guess\n",
        "            \n",
        "            lon_dropdown.disabled = False\n",
        "            lat_dropdown.disabled = False\n",
        "            date_dropdown.disabled = False\n",
        "            \n",
        "            # Update variable options (exclude coordinate and date columns)\n",
        "            excluded = {lon_guess, lat_guess, date_guess}\n",
        "            variable_options = [col for col in columns if col not in excluded and analysis.get(col, {}).get('type') == 'numeric']\n",
        "            \n",
        "            variable_select.options = variable_options\n",
        "            if variable_options:\n",
        "                # Select first few good quality variables\n",
        "                good_vars = [var for var in variable_options[:5] if analysis.get(var, {}).get('missing_pct', 100) < 50]\n",
        "                variable_select.value = tuple(good_vars[:3])  # Select up to 3 good variables\n",
        "            \n",
        "            variable_select.disabled = False\n",
        "            \n",
        "            # Update preview\n",
        "            update_variable_preview(analysis)\n",
        "            \n",
        "            print(f\"‚úÖ Found {len(variable_options)} numeric variables suitable for interpolation\")\n",
        "            \n",
        "            status_widget.value = (\n",
        "                \"<div style='padding: 10px; background: #e8f5e8; border-radius: 5px; margin: 10px 0;'>\"\n",
        "                f\"<b>‚úÖ CSV Loaded:</b> {csv_path.name}<br>\"\n",
        "                f\"üìä {len(columns)} total columns, {len(variable_options)} numeric variables<br>\"\n",
        "                f\"üéØ Auto-selected: {lon_guess or 'None'} (lon), {lat_guess or 'None'} (lat), {date_guess or 'None'} (date)\"\n",
        "                \"</div>\"\n",
        "            )\n",
        "            \n",
        "        except Exception as exc:\n",
        "            print(f\"‚ùå Failed to load CSV: {exc}\")\n",
        "            status_widget.value = (\n",
        "                \"<div style='padding: 10px; background: #ffebee; border-radius: 5px; margin: 10px 0;'>\"\n",
        "                f\"<b>‚ùå Error:</b> Failed to load CSV file<br>{str(exc)}</div>\"\n",
        "            )\n",
        "\n",
        "def preview_selection(_):\n",
        "    \"\"\"Preview the current selection before running interpolation\"\"\"\n",
        "    with output_area:\n",
        "        clear_output()\n",
        "        print(\"üëÄ SELECTION PREVIEW\")\n",
        "        print(\"=\" * 50)\n",
        "        \n",
        "        if not csv_path_widget.value:\n",
        "            print(\"‚ùå No CSV file selected\")\n",
        "            return\n",
        "            \n",
        "        if not variable_select.value:\n",
        "            print(\"‚ùå No variables selected\")\n",
        "            return\n",
        "            \n",
        "        print(f\"üìÅ Input File: {Path(csv_path_widget.value).name}\")\n",
        "        print(f\"üåç Coordinates: {lon_dropdown.value} (lon) √ó {lat_dropdown.value} (lat)\")\n",
        "        print(f\"üìÖ Date Column: {date_dropdown.value}\")\n",
        "        print(f\"üìä Variables: {len(variable_select.value)} selected\")\n",
        "        \n",
        "        for i, var in enumerate(variable_select.value, 1):\n",
        "            print(f\"   {i}. {var}\")\n",
        "            \n",
        "        print(f\"‚öôÔ∏è Method: {dict(method_dropdown.options)[method_dropdown.value]}\")\n",
        "        print(f\"üìè Cell Size: {cell_size_input.value} meters\")\n",
        "        print(f\"üîÑ CPU Jobs: {jobs_input.value}\")\n",
        "        \n",
        "        if output_path_widget.value:\n",
        "            print(f\"üìÇ Output: {output_path_widget.value}\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è No output directory selected\")\n",
        "\n",
        "def run_clicked(_):\n",
        "    \"\"\"Enhanced run function with progress tracking\"\"\"\n",
        "    # Validation\n",
        "    if not variable_select.value:\n",
        "        with output_area:\n",
        "            print(\"‚ùå Please select at least one variable\")\n",
        "        status_widget.value = (\n",
        "            \"<div style='padding: 10px; background: #fff3e0; border-radius: 5px; margin: 10px 0;'>\"\n",
        "            \"<b>‚ö†Ô∏è Warning:</b> Please select variables for interpolation</div>\"\n",
        "        )\n",
        "        return\n",
        "        \n",
        "    if not output_path_widget.value:\n",
        "        with output_area:\n",
        "            print(\"‚ùå Please specify an output directory\")\n",
        "        status_widget.value = (\n",
        "            \"<div style='padding: 10px; background: #fff3e0; border-radius: 5px; margin: 10px 0;'>\"\n",
        "            \"<b>‚ö†Ô∏è Warning:</b> Please select an output directory</div>\"\n",
        "        )\n",
        "        return\n",
        "    \n",
        "    # Disable controls during processing\n",
        "    run_button.disabled = True\n",
        "    preview_button.disabled = True\n",
        "    progress_bar.value = 0\n",
        "    \n",
        "    with output_area:\n",
        "        clear_output()\n",
        "        print(\"üöÄ STARTING CHEAQI INTERPOLATION\")\n",
        "        print(\"=\" * 50)\n",
        "        \n",
        "        status_widget.value = (\n",
        "            \"<div style='padding: 10px; background: #e3f2fd; border-radius: 5px; margin: 10px 0;'>\"\n",
        "            \"<b>üîÑ Processing:</b> Running spatial interpolation...</div>\"\n",
        "        )\n",
        "        \n",
        "        try:\n",
        "            progress_bar.value = 20\n",
        "            print(\"üìä Processing workflow...\")\n",
        "            \n",
        "            # Call the main processing function\n",
        "            process_workflow(\n",
        "                csv_path=csv_path_widget.value,\n",
        "                aoi_path=aoi_path_widget.value or None,\n",
        "                lon_col=lon_dropdown.value,\n",
        "                lat_col=lat_dropdown.value, \n",
        "                date_col=date_dropdown.value,\n",
        "                variable_cols=list(variable_select.value),\n",
        "                method=method_dropdown.value,\n",
        "                cell_size=cell_size_input.value,\n",
        "                power=power_input.value,\n",
        "                jobs=jobs_input.value,\n",
        "                output_folder=output_path_widget.value,\n",
        "            )\n",
        "            \n",
        "            progress_bar.value = 100\n",
        "            \n",
        "            status_widget.value = (\n",
        "                \"<div style='padding: 10px; background: #e8f5e8; border-radius: 5px; margin: 10px 0;'>\"\n",
        "                \"<b>‚úÖ Success:</b> Interpolation completed successfully!<br>\"\n",
        "                f\"üìÇ Results saved to: {Path(output_path_widget.value).name}</div>\"\n",
        "            )\n",
        "            \n",
        "            print(\"\\n\" + \"=\" * 50)\n",
        "            print(\"‚úÖ INTERPOLATION COMPLETED SUCCESSFULLY!\")\n",
        "            \n",
        "        except Exception as exc:\n",
        "            progress_bar.value = 0\n",
        "            print(f\"\\n‚ùå ERROR: {exc}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            \n",
        "            status_widget.value = (\n",
        "                \"<div style='padding: 10px; background: #ffebee; border-radius: 5px; margin: 10px 0;'>\"\n",
        "                f\"<b>‚ùå Error:</b> Interpolation failed<br>{str(exc)}</div>\"\n",
        "            )\n",
        "    \n",
        "    # Re-enable controls\n",
        "    run_button.disabled = False\n",
        "    preview_button.disabled = False\n",
        "\n",
        "# Connect event handlers\n",
        "csv_browse_button.on_click(browse_csv)\n",
        "aoi_browse_button.on_click(browse_aoi)\n",
        "output_browse_button.on_click(browse_output)\n",
        "load_columns_button.on_click(load_columns)\n",
        "preview_button.on_click(preview_selection)\n",
        "run_button.on_click(run_clicked)\n",
        "\n",
        "# Create organized layout with better spacing\n",
        "header_section = widgets.VBox([\n",
        "    status_widget,\n",
        "    widgets.HTML(\"<h3 style='color: #1976d2; margin: 20px 0 10px 0;'>üìÅ File Selection</h3>\"),\n",
        "    widgets.HBox([csv_path_widget, csv_browse_button]),\n",
        "    widgets.HBox([aoi_path_widget, aoi_browse_button]),\n",
        "    load_columns_button\n",
        "])\n",
        "\n",
        "column_section = widgets.VBox([\n",
        "    widgets.HTML(\"<h3 style='color: #1976d2; margin: 20px 0 10px 0;'>üéØ Column Configuration</h3>\"),\n",
        "    widgets.HBox([lon_dropdown, lat_dropdown, date_dropdown])\n",
        "])\n",
        "\n",
        "variable_section = widgets.VBox([\n",
        "    widgets.HTML(\"<h3 style='color: #1976d2; margin: 20px 0 10px 0;'>üìä Variable Selection</h3>\"),\n",
        "    widgets.HBox([variable_select, variable_preview])\n",
        "])\n",
        "\n",
        "method_section = widgets.VBox([\n",
        "    widgets.HTML(\"<h3 style='color: #1976d2; margin: 20px 0 10px 0;'>‚öôÔ∏è Interpolation Settings</h3>\"),\n",
        "    method_dropdown,\n",
        "    widgets.HBox([cell_size_input, power_input]),\n",
        "    widgets.HBox([jobs_input, quality_check])\n",
        "])\n",
        "\n",
        "output_section = widgets.VBox([\n",
        "    widgets.HTML(\"<h3 style='color: #1976d2; margin: 20px 0 10px 0;'>üìÇ Output Configuration</h3>\"),\n",
        "    widgets.HBox([output_path_widget, output_browse_button])\n",
        "])\n",
        "\n",
        "action_section = widgets.VBox([\n",
        "    widgets.HTML(\"<h3 style='color: #1976d2; margin: 20px 0 10px 0;'>üöÄ Actions</h3>\"),\n",
        "    widgets.HBox([preview_button, run_button]),\n",
        "    progress_bar\n",
        "])\n",
        "\n",
        "# Main interface layout\n",
        "main_interface = widgets.VBox([\n",
        "    widgets.HTML(\n",
        "        \"<div style='text-align: center; padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); \"\n",
        "        \"color: white; border-radius: 10px; margin-bottom: 20px;'>\"\n",
        "        \"<h1>üåç CHEAQI Spatial Interpolation Workbench</h1>\"\n",
        "        \"<p style='font-size: 16px; margin: 10px 0;'>Interactive Environmental Data Processing & Spatial Analysis</p>\"\n",
        "        \"</div>\"\n",
        "    ),\n",
        "    header_section,\n",
        "    column_section, \n",
        "    variable_section,\n",
        "    method_section,\n",
        "    output_section,\n",
        "    action_section,\n",
        "    widgets.HTML(\"<h3 style='color: #1976d2; margin: 20px 0 10px 0;'>üìã Processing Log</h3>\"),\n",
        "    output_area\n",
        "])\n",
        "\n",
        "# Display the enhanced interface\n",
        "display(main_interface)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c04e3284",
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_workflow(csv_path, aoi_path, lon_col, lat_col, date_col, variable_cols, \n",
        "                   method, cell_size=1000, power=2.0, jobs=4, output_folder=None):\n",
        "    \"\"\"\n",
        "    Enhanced processing workflow with proper error handling and progress tracking\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "    import geopandas as gpd\n",
        "    from pathlib import Path\n",
        "    import numpy as np\n",
        "    from datetime import datetime\n",
        "    import warnings\n",
        "    warnings.filterwarnings('ignore')\n",
        "    \n",
        "    # Setup paths\n",
        "    csv_path = Path(csv_path)\n",
        "    if not csv_path.exists():\n",
        "        raise FileNotFoundError(f\"CSV file not found: {csv_path}\")\n",
        "    \n",
        "    if output_folder is None:\n",
        "        output_folder = csv_path.parent / f\"cheaqi_outputs_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "    \n",
        "    output_folder = Path(output_folder)\n",
        "    output_folder.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    print(f\"üìÅ Input: {csv_path.name}\")\n",
        "    print(f\"üìÇ Output: {output_folder}\")\n",
        "    print(f\"üìä Variables: {len(variable_cols)} selected\")\n",
        "    print(f\"‚öôÔ∏è Method: {method}\")\n",
        "    \n",
        "    # Load and validate data\n",
        "    print(\"\\nüîç Loading and validating data...\")\n",
        "    df = pd.read_csv(csv_path)\n",
        "    \n",
        "    # Check required columns\n",
        "    missing_cols = []\n",
        "    if lon_col not in df.columns:\n",
        "        missing_cols.append(lon_col)\n",
        "    if lat_col not in df.columns:\n",
        "        missing_cols.append(lat_col)\n",
        "    if date_col not in df.columns:\n",
        "        missing_cols.append(date_col)\n",
        "    \n",
        "    for var in variable_cols:\n",
        "        if var not in df.columns:\n",
        "            missing_cols.append(var)\n",
        "    \n",
        "    if missing_cols:\n",
        "        raise ValueError(f\"Missing columns in CSV: {missing_cols}\")\n",
        "    \n",
        "    # Clean and validate coordinates\n",
        "    original_count = len(df)\n",
        "    df = df.dropna(subset=[lon_col, lat_col])\n",
        "    \n",
        "    # Convert coordinates to numeric\n",
        "    df[lon_col] = pd.to_numeric(df[lon_col], errors='coerce')\n",
        "    df[lat_col] = pd.to_numeric(df[lat_col], errors='coerce')\n",
        "    \n",
        "    # Remove invalid coordinates\n",
        "    valid_coords = (\n",
        "        (df[lon_col] >= -180) & (df[lon_col] <= 180) &\n",
        "        (df[lat_col] >= -90) & (df[lat_col] <= 90) &\n",
        "        df[lon_col].notna() & df[lat_col].notna()\n",
        "    )\n",
        "    df = df[valid_coords]\n",
        "    \n",
        "    cleaned_count = len(df)\n",
        "    print(f\"   üìä Data points: {original_count} ‚Üí {cleaned_count} (after cleaning)\")\n",
        "    \n",
        "    if cleaned_count == 0:\n",
        "        raise ValueError(\"No valid coordinate data found after cleaning\")\n",
        "    \n",
        "    # Process each variable\n",
        "    results_summary = []\n",
        "    \n",
        "    for i, variable in enumerate(variable_cols, 1):\n",
        "        progress_bar.value = int(20 + (i / len(variable_cols)) * 60)  # 20-80% range\n",
        "        \n",
        "        print(f\"\\nüîÑ Processing variable {i}/{len(variable_cols)}: {variable}\")\n",
        "        \n",
        "        try:\n",
        "            # Clean variable data\n",
        "            var_df = df[[lon_col, lat_col, date_col, variable]].copy()\n",
        "            var_df[variable] = pd.to_numeric(var_df[variable], errors='coerce')\n",
        "            var_df = var_df.dropna(subset=[variable])\n",
        "            \n",
        "            if len(var_df) == 0:\n",
        "                print(f\"   ‚ö†Ô∏è No valid data for {variable}\")\n",
        "                continue\n",
        "                \n",
        "            # Basic statistics\n",
        "            stats = {\n",
        "                'count': len(var_df),\n",
        "                'min': var_df[variable].min(),\n",
        "                'max': var_df[variable].max(), \n",
        "                'mean': var_df[variable].mean(),\n",
        "                'std': var_df[variable].std()\n",
        "            }\n",
        "            \n",
        "            print(f\"   üìà Stats: {stats['count']} points, range: {stats['min']:.3f} to {stats['max']:.3f}\")\n",
        "            \n",
        "            # Save processed data\n",
        "            processed_file = output_folder / f\"{variable}_processed_data.csv\"\n",
        "            var_df.to_csv(processed_file, index=False)\n",
        "            \n",
        "            # Create interpolation based on method\n",
        "            output_file = output_folder / f\"{variable}_{method}_interpolated.tif\"\n",
        "            \n",
        "            if method == \"gdal_grid\":\n",
        "                result = interpolate_gdal_idw(var_df, lon_col, lat_col, variable, \n",
        "                                           output_file, cell_size, power, aoi_path)\n",
        "            elif method == \"python_idw_kdtree\":\n",
        "                result = interpolate_python_kdtree(var_df, lon_col, lat_col, variable,\n",
        "                                                 output_file, cell_size, power, aoi_path)\n",
        "            elif method == \"pykrige_ok\":\n",
        "                result = interpolate_pykrige_ok(var_df, lon_col, lat_col, variable,\n",
        "                                              output_file, cell_size, aoi_path)\n",
        "            else:\n",
        "                print(f\"   ‚ùå Unknown method: {method}\")\n",
        "                continue\n",
        "            \n",
        "            # Store results\n",
        "            result_info = {\n",
        "                'variable': variable,\n",
        "                'method': method,\n",
        "                'data_points': stats['count'],\n",
        "                'output_file': output_file.name,\n",
        "                'stats': stats,\n",
        "                'success': True\n",
        "            }\n",
        "            \n",
        "            if result:\n",
        "                result_info.update(result)\n",
        "            \n",
        "            results_summary.append(result_info)\n",
        "            print(f\"   ‚úÖ Completed: {output_file.name}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Failed: {str(e)}\")\n",
        "            results_summary.append({\n",
        "                'variable': variable,\n",
        "                'method': method, \n",
        "                'error': str(e),\n",
        "                'success': False\n",
        "            })\n",
        "    \n",
        "    # Generate summary report\n",
        "    progress_bar.value = 90\n",
        "    print(f\"\\nüìã Generating summary report...\")\n",
        "    \n",
        "    summary_file = output_folder / \"interpolation_summary.json\"\n",
        "    \n",
        "    report = {\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'input_file': str(csv_path),\n",
        "        'output_folder': str(output_folder),\n",
        "        'method': method,\n",
        "        'parameters': {\n",
        "            'cell_size': cell_size,\n",
        "            'power': power,\n",
        "            'jobs': jobs\n",
        "        },\n",
        "        'coordinate_columns': {\n",
        "            'longitude': lon_col,\n",
        "            'latitude': lat_col,\n",
        "            'date': date_col\n",
        "        },\n",
        "        'total_variables': len(variable_cols),\n",
        "        'successful_interpolations': sum(1 for r in results_summary if r.get('success', False)),\n",
        "        'failed_interpolations': sum(1 for r in results_summary if not r.get('success', False)),\n",
        "        'results': results_summary\n",
        "    }\n",
        "    \n",
        "    with open(summary_file, 'w') as f:\n",
        "        json.dump(report, f, indent=2, default=str)\n",
        "    \n",
        "    # Create simple HTML report\n",
        "    html_file = output_folder / \"interpolation_report.html\"\n",
        "    create_html_report(report, html_file)\n",
        "    \n",
        "    progress_bar.value = 100\n",
        "    \n",
        "    print(f\"\\n‚úÖ Processing completed!\")\n",
        "    print(f\"üìä Success: {report['successful_interpolations']}/{report['total_variables']} variables\")\n",
        "    print(f\"üìÇ Results saved to: {output_folder}\")\n",
        "    print(f\"üìã Summary: {summary_file.name}\")\n",
        "    print(f\"üåê Report: {html_file.name}\")\n",
        "    \n",
        "    return report\n",
        "\n",
        "\n",
        "def interpolate_gdal_idw(df, lon_col, lat_col, var_col, output_file, cell_size=1000, power=2.0, aoi_path=None):\n",
        "    \"\"\"GDAL IDW interpolation with enhanced error handling\"\"\"\n",
        "    try:\n",
        "        from osgeo import gdal, ogr, osr\n",
        "        import tempfile\n",
        "        \n",
        "        print(f\"   üîß Running GDAL IDW interpolation...\")\n",
        "        \n",
        "        # Create temporary VRT file\n",
        "        with tempfile.NamedTemporaryFile(mode='w', suffix='.vrt', delete=False) as vrt_file:\n",
        "            vrt_content = f\"\"\"<OGRVRTDataSource>\n",
        "    <OGRVRTLayer name=\"points\">\n",
        "        <SrcDataSource>{df.to_csv(index=False)}</SrcDataSource>\n",
        "        <GeometryType>wkbPoint</GeometryType>\n",
        "        <LayerSRS>EPSG:4326</LayerSRS>\n",
        "        <GeometryField encoding=\"PointFromColumns\" x=\"{lon_col}\" y=\"{lat_col}\"/>\n",
        "    </OGRVRTLayer>\n",
        "</OGRVRTDataSource>\"\"\"\n",
        "            vrt_file.write(vrt_content)\n",
        "            vrt_path = vrt_file.name\n",
        "        \n",
        "        # Calculate extent\n",
        "        bounds = [df[lon_col].min(), df[lat_col].min(), df[lon_col].max(), df[lat_col].max()]\n",
        "        \n",
        "        # Build GDAL grid command\n",
        "        options = gdal.GridOptions(\n",
        "            algorithm=f'invdist:power={power}',\n",
        "            outputBounds=bounds,\n",
        "            width=int((bounds[2] - bounds[0]) * 111320 / cell_size),\n",
        "            height=int((bounds[3] - bounds[1]) * 111320 / cell_size),\n",
        "            outputSRS='EPSG:4326',\n",
        "            layers=['points'],\n",
        "            zfield=var_col,\n",
        "            format='GTiff'\n",
        "        )\n",
        "        \n",
        "        # Run interpolation\n",
        "        gdal.Grid(str(output_file), vrt_path, options=options)\n",
        "        \n",
        "        # Cleanup\n",
        "        Path(vrt_path).unlink(missing_ok=True)\n",
        "        \n",
        "        if output_file.exists():\n",
        "            return {'method_details': f'GDAL IDW (power={power}, cell_size={cell_size}m)'}\n",
        "        else:\n",
        "            raise RuntimeError(\"GDAL interpolation failed - no output file created\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"GDAL IDW failed: {str(e)}\")\n",
        "\n",
        "\n",
        "def interpolate_python_kdtree(df, lon_col, lat_col, var_col, output_file, cell_size=1000, power=2.0, aoi_path=None):\n",
        "    \"\"\"Python-based IDW using KDTree for fast nearest neighbor search\"\"\"\n",
        "    try:\n",
        "        from sklearn.neighbors import KDTree\n",
        "        import numpy as np\n",
        "        from osgeo import gdal, osr\n",
        "        \n",
        "        print(f\"   üêç Running Python KDTree IDW interpolation...\")\n",
        "        \n",
        "        # Prepare data\n",
        "        coords = df[[lon_col, lat_col]].values\n",
        "        values = df[var_col].values\n",
        "        \n",
        "        # Build KDTree\n",
        "        tree = KDTree(coords)\n",
        "        \n",
        "        # Create grid\n",
        "        bounds = [df[lon_col].min(), df[lat_col].min(), df[lon_col].max(), df[lat_col].max()]\n",
        "        \n",
        "        # Calculate grid dimensions\n",
        "        width = int((bounds[2] - bounds[0]) * 111320 / cell_size)\n",
        "        height = int((bounds[3] - bounds[1]) * 111320 / cell_size)\n",
        "        \n",
        "        x = np.linspace(bounds[0], bounds[2], width)\n",
        "        y = np.linspace(bounds[1], bounds[3], height)\n",
        "        xx, yy = np.meshgrid(x, y)\n",
        "        \n",
        "        grid_points = np.column_stack([xx.ravel(), yy.ravel()])\n",
        "        \n",
        "        # Find nearest neighbors and interpolate\n",
        "        k = min(10, len(coords))  # Use up to 10 nearest neighbors\n",
        "        distances, indices = tree.query(grid_points, k=k)\n",
        "        \n",
        "        # IDW calculation\n",
        "        with np.errstate(divide='ignore', invalid='ignore'):\n",
        "            weights = 1.0 / np.power(distances, power)\n",
        "            weights[distances == 0] = 1e16  # Handle exact matches\n",
        "            \n",
        "            weighted_values = weights * values[indices]\n",
        "            interpolated = np.sum(weighted_values, axis=1) / np.sum(weights, axis=1)\n",
        "        \n",
        "        # Reshape to grid\n",
        "        grid = interpolated.reshape(height, width)\n",
        "        \n",
        "        # Save as GeoTIFF\n",
        "        driver = gdal.GetDriverByName('GTiff')\n",
        "        dataset = driver.Create(str(output_file), width, height, 1, gdal.GDT_Float32)\n",
        "        \n",
        "        # Set geotransform\n",
        "        geotransform = (bounds[0], (bounds[2] - bounds[0]) / width, 0,\n",
        "                       bounds[3], 0, -(bounds[3] - bounds[1]) / height)\n",
        "        dataset.SetGeoTransform(geotransform)\n",
        "        \n",
        "        # Set projection\n",
        "        srs = osr.SpatialReference()\n",
        "        srs.ImportFromEPSG(4326)\n",
        "        dataset.SetProjection(srs.ExportToWkt())\n",
        "        \n",
        "        # Write data\n",
        "        band = dataset.GetRasterBand(1)\n",
        "        band.WriteArray(np.flipud(grid))  # Flip vertically for correct orientation\n",
        "        band.SetNoDataValue(-9999)\n",
        "        \n",
        "        dataset = None  # Close file\n",
        "        \n",
        "        return {'method_details': f'Python KDTree IDW (power={power}, neighbors={k})'}\n",
        "        \n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Python IDW failed: {str(e)}\")\n",
        "\n",
        "\n",
        "def interpolate_pykrige_ok(df, lon_col, lat_col, var_col, output_file, cell_size=1000, aoi_path=None):\n",
        "    \"\"\"PyKrige Ordinary Kriging interpolation\"\"\"\n",
        "    try:\n",
        "        from pykrige.ok import OrdinaryKriging\n",
        "        import numpy as np\n",
        "        from osgeo import gdal, osr\n",
        "        \n",
        "        print(f\"   üìà Running PyKrige Ordinary Kriging...\")\n",
        "        \n",
        "        # Prepare data\n",
        "        x = df[lon_col].values\n",
        "        y = df[lat_col].values\n",
        "        z = df[var_col].values\n",
        "        \n",
        "        # Create kriging object\n",
        "        ok = OrdinaryKriging(x, y, z, variogram_model='linear', verbose=False)\n",
        "        \n",
        "        # Create grid\n",
        "        bounds = [x.min(), y.min(), x.max(), y.max()]\n",
        "        \n",
        "        # Calculate grid dimensions  \n",
        "        width = int((bounds[2] - bounds[0]) * 111320 / cell_size)\n",
        "        height = int((bounds[3] - bounds[1]) * 111320 / cell_size)\n",
        "        \n",
        "        grid_x = np.linspace(bounds[0], bounds[2], width)\n",
        "        grid_y = np.linspace(bounds[1], bounds[3], height)\n",
        "        \n",
        "        # Perform kriging\n",
        "        z_pred, ss = ok.execute('grid', grid_x, grid_y)\n",
        "        \n",
        "        # Save as GeoTIFF\n",
        "        driver = gdal.GetDriverByName('GTiff')\n",
        "        dataset = driver.Create(str(output_file), width, height, 1, gdal.GDT_Float32)\n",
        "        \n",
        "        # Set geotransform\n",
        "        geotransform = (bounds[0], (bounds[2] - bounds[0]) / width, 0,\n",
        "                       bounds[3], 0, -(bounds[3] - bounds[1]) / height)\n",
        "        dataset.SetGeoTransform(geotransform)\n",
        "        \n",
        "        # Set projection\n",
        "        srs = osr.SpatialReference()\n",
        "        srs.ImportFromEPSG(4326)\n",
        "        dataset.SetProjection(srs.ExportToWkt())\n",
        "        \n",
        "        # Write data\n",
        "        band = dataset.GetRasterBand(1)\n",
        "        band.WriteArray(z_pred)\n",
        "        band.SetNoDataValue(-9999)\n",
        "        \n",
        "        dataset = None  # Close file\n",
        "        \n",
        "        return {\n",
        "            'method_details': f'PyKrige Ordinary Kriging (variogram=linear)',\n",
        "            'kriging_variance': float(np.mean(ss))\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"PyKrige failed: {str(e)}\")\n",
        "\n",
        "\n",
        "def create_html_report(report, output_file):\n",
        "    \"\"\"Create a simple HTML report\"\"\"\n",
        "    html_content = f\"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>CHEAQI Interpolation Report</title>\n",
        "    <style>\n",
        "        body {{ font-family: Arial, sans-serif; margin: 20px; }}\n",
        "        .header {{ background: #4CAF50; color: white; padding: 20px; border-radius: 5px; }}\n",
        "        .section {{ margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }}\n",
        "        .success {{ color: #4CAF50; }}\n",
        "        .error {{ color: #f44336; }}\n",
        "        table {{ border-collapse: collapse; width: 100%; }}\n",
        "        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
        "        th {{ background-color: #f2f2f2; }}\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"header\">\n",
        "        <h1>üåç CHEAQI Spatial Interpolation Report</h1>\n",
        "        <p>Generated: {report['timestamp']}</p>\n",
        "    </div>\n",
        "    \n",
        "    <div class=\"section\">\n",
        "        <h2>üìä Summary</h2>\n",
        "        <p><strong>Input File:</strong> {report['input_file']}</p>\n",
        "        <p><strong>Method:</strong> {report['method']}</p>\n",
        "        <p><strong>Variables Processed:</strong> {report['total_variables']}</p>\n",
        "        <p class=\"success\"><strong>Successful:</strong> {report['successful_interpolations']}</p>\n",
        "        <p class=\"error\"><strong>Failed:</strong> {report['failed_interpolations']}</p>\n",
        "    </div>\n",
        "    \n",
        "    <div class=\"section\">\n",
        "        <h2>üîß Parameters</h2>\n",
        "        <ul>\n",
        "            <li>Cell Size: {report['parameters']['cell_size']} meters</li>\n",
        "            <li>IDW Power: {report['parameters']['power']}</li>\n",
        "            <li>CPU Jobs: {report['parameters']['jobs']}</li>\n",
        "        </ul>\n",
        "    </div>\n",
        "    \n",
        "    <div class=\"section\">\n",
        "        <h2>üìã Detailed Results</h2>\n",
        "        <table>\n",
        "            <tr>\n",
        "                <th>Variable</th>\n",
        "                <th>Status</th>\n",
        "                <th>Data Points</th>\n",
        "                <th>Output File</th>\n",
        "            </tr>\n",
        "\"\"\"\n",
        "    \n",
        "    for result in report['results']:\n",
        "        status = \"‚úÖ Success\" if result.get('success', False) else \"‚ùå Failed\"\n",
        "        status_class = \"success\" if result.get('success', False) else \"error\"\n",
        "        points = result.get('data_points', 'N/A')\n",
        "        output = result.get('output_file', result.get('error', 'N/A'))\n",
        "        \n",
        "        html_content += f\"\"\"\n",
        "            <tr>\n",
        "                <td>{result['variable']}</td>\n",
        "                <td class=\"{status_class}\">{status}</td>\n",
        "                <td>{points}</td>\n",
        "                <td>{output}</td>\n",
        "            </tr>\n",
        "\"\"\"\n",
        "    \n",
        "    html_content += \"\"\"\n",
        "        </table>\n",
        "    </div>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "    \n",
        "    with open(output_file, 'w') as f:\n",
        "        f.write(html_content)\n",
        "\n",
        "print(\"‚úÖ Enhanced processing functions loaded and ready!\")\n",
        "print(\"üéØ Action buttons are now fully functional with:\")\n",
        "print(\"   ‚Ä¢ Smart file dialogs with error handling\")  \n",
        "print(\"   ‚Ä¢ Comprehensive CSV analysis and column detection\")\n",
        "print(\"   ‚Ä¢ Real-time variable preview with quality assessment\") \n",
        "print(\"   ‚Ä¢ Progress tracking during interpolation\")\n",
        "print(\"   ‚Ä¢ Detailed results summary and HTML reports\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "583dbe36",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üåç CHEAQI Interactive Workbench Ready!\n",
            "============================================================\n",
            "üìã INSTRUCTIONS:\n",
            "1. üöÄ Run the cells above to load the interface\n",
            "2. üìÅ Click 'Browse Files' to select your CSV data file\n",
            "3. üîÑ Click 'Load & Analyze CSV' to detect columns automatically\n",
            "4. üìä Select variables from the list (multiple selection enabled)\n",
            "5. ‚öôÔ∏è Choose interpolation method and adjust parameters\n",
            "6. üìÇ Select output directory for results\n",
            "7. üëÄ Use 'Preview Selection' to verify settings\n",
            "8. üöÄ Click 'Run Interpolation' to start processing\n",
            "\n",
            "‚ú® FEATURES:\n",
            "‚Ä¢ Smart column detection (longitude, latitude, date)\n",
            "‚Ä¢ Variable quality assessment with missing data analysis\n",
            "‚Ä¢ Multiple interpolation methods (GDAL IDW, Python KDTree, PyKrige)\n",
            "‚Ä¢ Real-time progress tracking\n",
            "‚Ä¢ Comprehensive HTML and JSON reports\n",
            "‚Ä¢ Error handling with detailed feedback\n",
            "\n",
            "üéØ Ready to process environmental data! Select your CSV file to begin.\n",
            "‚úÖ All required packages loaded successfully\n"
          ]
        }
      ],
      "source": [
        "# Quick Test and Instructions\n",
        "print(\"üåç CHEAQI Interactive Workbench Ready!\")\n",
        "print(\"=\" * 60)\n",
        "print(\"üìã INSTRUCTIONS:\")\n",
        "print(\"1. üöÄ Run the cells above to load the interface\")\n",
        "print(\"2. üìÅ Click 'Browse Files' to select your CSV data file\")  \n",
        "print(\"3. üîÑ Click 'Load & Analyze CSV' to detect columns automatically\")\n",
        "print(\"4. üìä Select variables from the list (multiple selection enabled)\")\n",
        "print(\"5. ‚öôÔ∏è Choose interpolation method and adjust parameters\")\n",
        "print(\"6. üìÇ Select output directory for results\")\n",
        "print(\"7. üëÄ Use 'Preview Selection' to verify settings\")\n",
        "print(\"8. üöÄ Click 'Run Interpolation' to start processing\")\n",
        "print()\n",
        "print(\"‚ú® FEATURES:\")\n",
        "print(\"‚Ä¢ Smart column detection (longitude, latitude, date)\")\n",
        "print(\"‚Ä¢ Variable quality assessment with missing data analysis\")  \n",
        "print(\"‚Ä¢ Multiple interpolation methods (GDAL IDW, Python KDTree, PyKrige)\")\n",
        "print(\"‚Ä¢ Real-time progress tracking\")\n",
        "print(\"‚Ä¢ Comprehensive HTML and JSON reports\")\n",
        "print(\"‚Ä¢ Error handling with detailed feedback\")\n",
        "print()\n",
        "print(\"üéØ Ready to process environmental data! Select your CSV file to begin.\")\n",
        "\n",
        "# Test that key modules are available\n",
        "try:\n",
        "    import ipywidgets\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    print(\"‚úÖ All required packages loaded successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Missing package: {e}\")\n",
        "    print(\"üí° Install with: pip install ipywidgets pandas numpy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "192b9c7f",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "52fcb216",
      "metadata": {},
      "source": [
        "## Usage & Troubleshooting\n",
        "1. Use the **Browse** buttons to choose the CSV, optional AOI shapefile, and output directory, then press **Reload Columns**.\n",
        "2. Pick longitude, latitude, date, and one or more variables, configure interpolation settings (GDAL IDW, Python IDW, or PyKrige OK), and press **Run Interpolation**.\n",
        "3. Watch the log pane for progress; any Python errors or missing dependency warnings will appear there.\n",
        "4. After the dated GeoTIFFs are generated, the notebook assembles a master stack (`CHEAQI_master_stack.tif`), writes a NetCDF cube (`CHEAQI_timeseries.nc` when xarray is installed), and records both per-date and master band metadata in `CHEAQI_per_date_bandmap.csv`.\n",
        "5. If GDAL tools are missing, the notebook automatically falls back to the Python KDTree method (requires SciPy); PyKrige requires the `pykrige` package.\n",
        "6. When troubleshooting, confirm column names, inspect the AOI CRS, and ensure the output directory is writable.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
